{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apikeyspath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bc2d8b65d9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Get keys (requires a module named \"apikeyspath.py\" with your API key in the variable NYT_ARTICLE_SEARCH_KEY) and path to repository\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapikeyspath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNYT_ARTICLE_SEARCH_KEY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mapikeyspath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPATH_TO_REPO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apikeyspath'"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "import http.client\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "import time\n",
    "\n",
    "# Get keys (requires a module named \"apikeyspath.py\" with your API key in the variable NYT_ARTICLE_SEARCH_KEY) and path to repository\n",
    "# from apikeyspath import NYT_ARTICLE_SEARCH_KEY\n",
    "# from apikeyspath import PATH_TO_REPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_ARTICLE_SEARCH_KEY = \"xxxxxxxx\"\n",
    "PATH_TO_REPO = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Articles(object):\n",
    "    \"\"\"\n",
    "    Class that holds functions to fetch and store NYT articles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # List of all articles\n",
    "        self.all_articles = []\n",
    "\n",
    "\n",
    "    def fetch_articles(self, pages, items, sec_or_desk, begin_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetches articles from NYT Article Search API\n",
    "        pages (int): number of pages. Each page consists of 10 items\n",
    "        items (list of strings): list of items used as keywords to fetch articles from\n",
    "        sec_or_desk (boolean): If true, search in sections, otherwise in newsdesks\n",
    "        begin_date (int): Begin date as YYYYMMDD\n",
    "        end_date (int): End date as YYYYMMDD\n",
    "        returns nothing\n",
    "        \"\"\"\n",
    "        # Prepare search string for url\n",
    "        search = \"%22+%22\".join(items)\n",
    "\n",
    "        # Section or newsdesk search?\n",
    "        searchtype = \"section_name\" if sec_or_desk else \"news_desk\"\n",
    "\n",
    "        # List of all single alphabetic letters\n",
    "        singleletters = [chr(i) for i in range(97,123)] + [chr(i).upper() for i in range(97,123)]\n",
    "\n",
    "        # Counter for bad articles, which are not processed due to missing data\n",
    "        badcount = 0\n",
    "\n",
    "        # Run over all responses for all pages\n",
    "        for page in range(pages):\n",
    "\n",
    "            # There can be only made 10 calls per second to the NYT Article API, therefore wait for 0.1 seconds before each query\n",
    "            time.sleep(6)\n",
    "\n",
    "            # Get the data\n",
    "            request_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=\" + searchtype + \".contains%3A%28%22\" + search + \"%22%29&fl=web_url%2Csnippet%2Clead_paragraph%2Cabstract%2Cheadline%2Ckeywords%2Cpub_date%2Cdocument_type%2Cnews_desk%2Ctype_of_material&page=\" + str(page) + \"&begin_date=\" + str(begin_date) + \"&end_date=\" + str(end_date) + \"&api-key=\" + NYT_ARTICLE_SEARCH_KEY\n",
    "            try:\n",
    "                response = urllib.request.urlopen(request_url).read()\n",
    "            except (urllib.error.HTTPError):\n",
    "                print (\"Error code: \" + str(e.code))\n",
    "                print (\"Error message: \" + e.msg)\n",
    "                print (\"Error hdrs:\\n\" + str(e.hdrs))\n",
    "                sys.exit()\n",
    "\n",
    "            # Load json response into python dictionary and reorganize some data\n",
    "            articles = json.loads(response)\n",
    "            articles_smooth = articles[\"response\"][\"docs\"]\n",
    "            for i in range(len(articles_smooth)):\n",
    "\n",
    "                # Make header column. If this fails, skip the article\n",
    "                try:\n",
    "                    articles_smooth[i][\"header\"] = articles_smooth[i][\"headline\"][\"main\"]\n",
    "                except:\n",
    "                    badcount += 1\n",
    "                    print (\"Bad article #\" + str(badcount) + \", skip and continue...\")\n",
    "                    continue\n",
    "\n",
    "                # Make keywordlist column. If this fails, skip the article\n",
    "                try:\n",
    "                    articles_smooth[i][\"keywordlist\"] = \" \".join([item[\"value\"] for item in articles_smooth[i][\"keywords\"]])\n",
    "                except:\n",
    "                    badcount += 1\n",
    "                    print (\"Bad article #\" + str(badcount) + \", skip and continue...\")\n",
    "                    continue\n",
    "\n",
    "                # Make column with all word features. If this fails, skip the article. Also note that blogpost do not have the lead_paragraph feature\n",
    "                try:\n",
    "                    if articles_smooth[i][\"document_type\"] == \"blogpost\":\n",
    "                        articles_smooth[i][\"allwords\"] = \" \".join([articles_smooth[i][\"header\"], articles_smooth[i][\"keywordlist\"], articles_smooth[i][\"snippet\"]])\n",
    "                    else:\n",
    "                        articles_smooth[i][\"allwords\"] = \" \".join([articles_smooth[i][\"header\"], articles_smooth[i][\"keywordlist\"], articles_smooth[i][\"lead_paragraph\"], articles_smooth[i][\"snippet\"]])\n",
    "                except:\n",
    "                    badcount += 1\n",
    "                    print (\"Bad article #\" + str(badcount) + \", skip and continue...\")\n",
    "                    continue\n",
    "\n",
    "                # Clean all non alphabetic characters and throw away individual letters\n",
    "                wordlist = \"\".join( [char if char in singleletters else \" \" for char in articles_smooth[i][\"allwords\"]] ).split()\n",
    "                cleanwordlist = [word for word in wordlist if word not in singleletters]\n",
    "\n",
    "                # Join as string, convert into regular string, and copy back onto allwords\n",
    "                articles_smooth[i][\"allwords\"] = \" \".join(cleanwordlist)\n",
    "\n",
    "                # Delete old keywords and headline columns\n",
    "                del articles_smooth[i][\"keywords\"]\n",
    "                del articles_smooth[i][\"headline\"]\n",
    "\n",
    "            # Append to all articles\n",
    "            self.all_articles.extend(articles_smooth)\n",
    "\n",
    "\n",
    "    def write_articles(self, filename):\n",
    "        \"\"\"\n",
    "        Writes saved articles as json to disc\n",
    "        filename (string): filename with directory called \"Articles\"\n",
    "        returns nothing\n",
    "        \"\"\"\n",
    "        # Make folder for saving the data if it does not already exist\n",
    "        if not os.path.isdir(PATH_TO_REPO + \"articles\"):\n",
    "            cmd = \"mkdir {}articles\".format(PATH_TO_REPO)\n",
    "            os.system(cmd)\n",
    "\n",
    "        # And save the data\n",
    "        open(PATH_TO_REPO + \"articles/\" + filename + \".json\", \"w\").write(json.dumps(self.all_articles))\n",
    "\n",
    "\n",
    "    def clear_articles(self):\n",
    "        \"\"\"\n",
    "        Clears article list\n",
    "        returns nothing\n",
    "        \"\"\"\n",
    "        self.all_articles = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Section World...\n",
      "Scraping Section World done.\n",
      "\n",
      "Scraping Section US...\n",
      "Scraping Section US done.\n",
      "\n",
      "Scraping Section NY...\n",
      "Scraping Section NY done.\n",
      "\n",
      "Scraping Section Business...\n",
      "Scraping Section Business done.\n",
      "\n",
      "Scraping Section Tech...\n",
      "Scraping Section Tech done.\n",
      "\n",
      "Scraping Section Science...\n",
      "Scraping Section Science done.\n",
      "\n",
      "Scraping Section Health...\n",
      "Scraping Section Health done.\n",
      "\n",
      "Scraping Section Sports...\n",
      "Scraping Section Sports done.\n",
      "\n",
      "Scraping Section Arts...\n",
      "Scraping Section Arts done.\n",
      "\n",
      "Scraping Section Style...\n",
      "Scraping Section Style done.\n",
      "\n",
      "Scraping Section Food...\n",
      "Scraping Section Food done.\n",
      "\n",
      "Scraping Section Travel...\n",
      "Scraping Section Travel done.\n",
      "\n",
      "Scraping Section RealEstate...\n",
      "Scraping Section RealEstate done.\n",
      "\n",
      "Scraping Newsdesks Politics...\n",
      "Scraping Newsdesks Politics done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    # Get training data for politics class by search on news desks\n",
    "    CategoriesDesks = {\"Politics\" : [\"Politics\", \"Washington\"]}\n",
    "\n",
    "    # Get training data for all other classes by search on sections\n",
    "    CategoriesSections = {\"World\" : [\"World\"],\"US\" : [\"U.S.\"], \"NY\" : [\"N.Y.\", \"NY\", \"New+York\"], \"Business\" : [\"Business\"], \"Tech\" : [\"Technology\"], \"Science\" : [\"Science\"], \"Health\" : [\"Health\"], \"Sports\" : [\"Sports\"], \"Arts\" : [\"Arts\"], \"Style\" : [\"Style\"], \"Food\" : [\"Food\"], \"Travel\" : [\"Travel\"], \"RealEstate\" : [\"Real+Estate\"]}\n",
    "\n",
    "    # Initialize class, set pages as well as begindates and enddates\n",
    "    AllArticles = Articles()\n",
    "    pages = 1\n",
    "\n",
    "    # Define begin and end dates\n",
    "    begin_dates = [20150301, 20140901, 20140301, 20130901, 20130301, 20120901, 20120301, 20110901, 20110301, 20100901]\n",
    "    end_dates = [20150827, 20150228, 20140831, 20140228, 20130831, 20130228, 20120831, 20120229, 20110831, 20110228]\n",
    "\n",
    "    # Query the API and collect all articles for all categories (note that you typically cannot do this in one run, as the article API allows only 10.000 calls per day)\n",
    "    for key, sections in CategoriesSections.items():\n",
    "        print (\"Scraping Section \" + key + \"...\")\n",
    "        for start, end in zip(begin_dates, end_dates):\n",
    "            AllArticles.fetch_articles(pages = pages, items = sections, sec_or_desk = True, begin_date = start, end_date = end)\n",
    "            AllArticles.write_articles(filename = \"Articles_\" + key + \"_start\" + str(start) + \"_end\" + str(end))\n",
    "            AllArticles.clear_articles()\n",
    "        print (\"Scraping Section \" + key + \" done.\\n\")\n",
    "\n",
    "    for key, desks in CategoriesDesks.items():\n",
    "        print (\"Scraping Newsdesks \" + key + \"...\")\n",
    "        for start, end in zip(begin_dates, end_dates):\n",
    "            AllArticles.fetch_articles(pages = pages, items = desks, sec_or_desk = False, begin_date = start, end_date = end)\n",
    "            AllArticles.write_articles(filename = \"Articles_\" + key  + \"_start\" + str(start) + \"_end\" + str(end))\n",
    "            AllArticles.clear_articles()\n",
    "        print (\"Scraping Newsdesks \" + key + \" done.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['abstract', 'web_url', 'snippet', 'lead_paragraph', 'pub_date',\n",
      "       'document_type', 'news_desk', 'type_of_material', 'header',\n",
      "       'keywordlist', 'allwords', 'label'],\n",
      "      dtype='object')\n",
      "Index(['type_of_material', 'allwords'], dtype='object')\n",
      "dfdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
